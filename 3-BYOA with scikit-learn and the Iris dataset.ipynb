{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building your own Docker image for BYOA\n",
    "\n",
    "We will start our MLOps journey here by creating an abstract Docker Image for supporting Scikit-learn algorithms/models. As explained in https://docs.aws.amazon.com/sagemaker/latest/dg/amazon-sagemaker-containers.html SageMaker defines a common interface for its docker containers.\n",
    "\n",
    "## Let's create the Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM python:3.6-jessie\n",
    "\n",
    "RUN apt-get update -y && apt-get install -y libev-dev\n",
    "RUN pip install bottle bjoern opencv-python pandas==0.25.1 numpy scipy scikit-learn\n",
    "\n",
    "RUN mkdir -p /opt/program\n",
    "RUN mkdir -p /opt/ml\n",
    "\n",
    "ENV PYTHONUNBUFFERED=TRUE\n",
    "ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
    "ENV PATH=\"/opt/program:${PATH}\"\n",
    "\n",
    "COPY app.py /opt/program\n",
    "COPY model.py /opt/program\n",
    "WORKDIR /opt/program\n",
    "\n",
    "EXPOSE 8080\n",
    "ENTRYPOINT [\"python\", \"app.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the two default directories /opt/program and /opt/ml used by SageMaker containers\n",
    "\n",
    "Also, if running without arguments, the continer will start with `python app.py` on the /opt/program directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create our application code\n",
    "\n",
    "Please, notice that we're creating a WebService application with two methods: ping and invocations. Ping is for healthcheck and invocations is for calling your model. For a production environment it is important to use a WSGI solution. We will use a combo of bottle and bjoern. Bottle is our webservice api and bjoern our WSGI server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "import json\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "import bjoern\n",
    "import bottle\n",
    "\n",
    "from bottle import run, request, post, get\n",
    "import joblib\n",
    "\n",
    "# adds the model.py path to the list\n",
    "prefix = '/opt/ml'\n",
    "model_path = os.path.join(prefix, 'model')\n",
    "sys.path.insert(0,model_path)\n",
    "\n",
    "print(os.listdir(model_path))\n",
    "import model\n",
    "\n",
    "@get('/ping')\n",
    "def ping():\n",
    "    return \"\"\n",
    "\n",
    "@post('/invocations')\n",
    "def invoke():\n",
    "    # load image from POST and convert it to json\n",
    "    req = json.loads(request.body.read())\n",
    "    algo = \"logistic\" # request.get_header('X-Amzn-SageMaker-Custom-Attributes')\n",
    "    return json.dumps(model.predict(req, algo))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    if len(sys.argv) < 2 or ( not sys.argv[1] in [ \"serve\", \"train\", \"test\"] ):\n",
    "        raise Exception(\"Invalid argument: you must inform 'train' for training mode or 'serve' predicting mode\") \n",
    "\n",
    "    train = sys.argv[1] == \"train\"\n",
    "    test = sys.argv[1] == \"test\"\n",
    "    \n",
    "    if train:\n",
    "        model.train()\n",
    "\n",
    "    elif test:\n",
    "        algo = sys.argv[2]\n",
    "        req = eval(sys.argv[3])\n",
    "        print( model.predict(req, algo) )\n",
    "       \n",
    "    else:\n",
    "        bjoern.run(bottle.app(), \"0.0.0.0\", 8080)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create the model training and inference code using scikit-learn\n",
    "\n",
    "We now create the `model.py` module used by `app.py` above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model.py\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from sklearn import model_selection\n",
    "import joblib\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# This directory is the communication channel between Sagemaker and your container\n",
    "prefix = '/opt/ml'\n",
    "\n",
    "# Here, Sagemaker will store the dataset copyied from S3\n",
    "input_path = os.path.join(prefix, 'input/data')\n",
    "# If something bad happens, write a failure file with the error messages and store here\n",
    "output_path = os.path.join(prefix, 'output')\n",
    "# Everything you store here will be packed into a .tar.gz by Sagemaker and store into S3\n",
    "model_path = os.path.join(prefix, 'model')\n",
    "# This is the hyperparameters you will send to your algorithms through the Estimator\n",
    "param_path = os.path.join(prefix, 'input/config/hyperparameters.json')\n",
    "\n",
    "model_cache = {}\n",
    "\n",
    "def train():\n",
    "    print(\"Training mode\")\n",
    "    \n",
    "    try:\n",
    "        # This algorithm has a single channel of input data called 'training'. Since we run in\n",
    "        # File mode, the input files are copied to the directory specified here.\n",
    "        channel_name='training'\n",
    "        training_path = os.path.join(input_path, channel_name)\n",
    "\n",
    "        hyper_logistic = {}\n",
    "        hyper_random_forest = {}\n",
    "        # Read in any hyperparameters that the user passed with the training job\n",
    "        with open(param_path, 'r') as tc:\n",
    "            is_float = re.compile(r'^\\d+(?:\\.\\d+)$')\n",
    "            is_integer = re.compile(r'^\\d+$')\n",
    "            for key,value in json.load(tc).items():\n",
    "                # workaround to convert numbers from string\n",
    "                if is_float.match(value) is not None:\n",
    "                    value = float(value)\n",
    "                elif is_integer.match(value) is not None:\n",
    "                    value = int(value)\n",
    "                \n",
    "                if key.startswith('logistic'):\n",
    "                    key = key.replace('logistic_', '')\n",
    "                    hyper_logistic[key] = value\n",
    "\n",
    "        # Take the set of files and read them all into a single pandas dataframe\n",
    "        input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]\n",
    "        if len(input_files) == 0:\n",
    "            raise ValueError(('There are no files in {}.\\\\n' +\n",
    "                              'This usually indicates that the channel ({}) was incorrectly specified,\\\\n' +\n",
    "                              'the data specification in S3 was incorrectly specified or the role specified\\\\n' +\n",
    "                              'does not have permission to access the data.').format(training_path, channel_name))\n",
    "        raw_data = [ pd.read_csv(file, sep=',', header=None ) for file in input_files ]\n",
    "        train_data = pd.concat(raw_data)\n",
    "        \n",
    "        # labels are in the first column\n",
    "        Y = train_data.iloc[:,0]\n",
    "        X = train_data.iloc[:,1:]\n",
    "        \n",
    "        X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size=0.33, random_state=7)\n",
    "\n",
    "        algo = \"logistic\"\n",
    "        print(\"Training: %s\" % algo)\n",
    "        model = LogisticRegression()\n",
    "        model.set_params(**hyper_logistic)\n",
    "        model.fit(X_train, Y_train)\n",
    "        print(\"{}: {}\".format( algo, model.score(X_test, Y_test)) )\n",
    "        joblib.dump(model, open(os.path.join(model_path, '%s_model.pkl' % algo), 'wb'))\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Write out an error file. This will be returned as the failureReason in the\n",
    "        # DescribeTrainingJob result.\n",
    "        trc = traceback.format_exc()\n",
    "        with open(os.path.join(output_path, 'failure'), 'w') as s:\n",
    "            s.write('Exception during training: ' + str(e) + '\\\\n' + trc)\n",
    "            \n",
    "        # Printing this causes the exception to be in the training job logs, as well.\n",
    "        print('Exception during training: ' + str(e) + '\\\\n' + trc, file=sys.stderr)\n",
    "        \n",
    "        # A non-zero exit code causes the training job to be marked as Failed.\n",
    "        sys.exit(255)\n",
    "\n",
    "def predict(payload, algo):\n",
    "    if algo is None or payload is None:\n",
    "        raise ValueError( \"You need to inform the algorithm and the payload\" )\n",
    "    \n",
    "    if model_cache.get(algo) is None:\n",
    "        model_filename = os.path.join(model_path, '%s_model.pkl' % algo)\n",
    "        model_cache[algo] = joblib.load(open(model_filename, 'rb'))\n",
    "    \n",
    "    return {\"iris_id\": model_cache[algo].predict( payload ).tolist() }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test building the image locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo docker build -f Dockerfile -t iris-model:1.0 ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's do some tests with our model image, locally\n",
    "\n",
    "First let's define some hyperparameters and store them as a json file. Since our container is using logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"logistic_max_iter\": 100,\n",
    "    \"logistic_solver\": \"lbfgs\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we store the hyperparameters as a JSON file in an input folder that will be mapped to the folder SageMaker\n",
    "# expects to find them on\n",
    "import json\n",
    "!mkdir -p input/config\n",
    "\n",
    "hyperparameters = dict({key: str(values) for key, values in hyperparameters.items()})\n",
    "with open('input/config/hyperparameters.json', 'w') as f:\n",
    "    f.write(json.dumps(hyperparameters))\n",
    "    f.flush()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's copy the Iris training data file to the training channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p input/data/training\n",
    "!cp iris_train.csv input/data/training/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And we test the training process\n",
    "\n",
    "We will map the local notebook folders to the folders that SageMaker uses to sync data and models from S3 to the container and back, to simulate the run as it would be happening on SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"Training ...\")\n",
    "!docker run --rm --name 'my_model' \\\n",
    "    -v \"$PWD/model:/opt/ml/model\" \\\n",
    "    -v \"$PWD/input:/opt/ml/input\" iris-model:1.0 train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check your model folder\n",
    "\n",
    "You will see that the trained model has been created.\n",
    "\n",
    "## Let's test inference with the container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"Testing with logistic\")\n",
    "!docker run --rm --name 'my_model' \\\n",
    "    -v \"$PWD/model:/opt/ml/model\" \\\n",
    "    -v \"$PWD/input:/opt/ml/input\" iris-model:1.0 test logistic \"[[6.7, 3.1, 5.6, 2.4]]\"\n",
    "print(\"Result for iris type above should be category 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's simulate an Endpoint exposed by Sagemaker\n",
    "\n",
    "After you execute the next cell, this Jupyter notebook will freeze. A webservice will be exposed at the port 8080."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run --rm --name 'my_model' \\\n",
    "    -p 8080:8080 \\\n",
    "    -v \"$PWD/model:/opt/ml/model\" \\\n",
    "    -v \"$PWD/input:/opt/ml/input\" iris-model:1.0 serve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the above cell is running, you can open a terminal or a separate notebook to make POST requests for inference to port 8080, or to test the healthcheck /ping path.\n",
    "\n",
    "e.g. code in Python (note that no authorization is used at this point):\n",
    "```\n",
    "import json, requests\n",
    "\n",
    "payload = json.dumps([[4.6, 3.1, 1.5, 0.2]]).encode('utf-8')\n",
    "headers={\"Content-type\": \"application/json\", \"X-Amzn-SageMaker-Custom-Attributes\": \"logistic\"}\n",
    "\n",
    "resp = requests.post('http://localhost:8080/invocations', data = payload, headers = headers)\n",
    "print(\"Response code: %d, Payload: [%s]\" % (resp.status_code, resp.text))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can now BYOA to SageMaker\n",
    "\n",
    "We need to publish the container to ECR first, but it should be ready to work as a new custom SageMaker training and inference algorithm. Let's first stop the previous cell from running!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "os.environ['IMAGE_REPO_NAME']=\"iris-model\"\n",
    "os.environ['IMAGE_TAG']=\"1.0\"\n",
    "os.environ['AWS_DEFAULT_REGION']=region\n",
    "os.environ['AWS_ACCOUNT_ID']=boto3.client('sts').get_caller_identity()[\"Account\"]\n",
    "\n",
    "# Get login for ECR in current region\n",
    "!aws ecr create-repository --repository-name $IMAGE_REPO_NAME\n",
    "!$(aws ecr get-login --no-include-email --region $AWS_DEFAULT_REGION)\n",
    "!docker tag $IMAGE_REPO_NAME:$IMAGE_TAG $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG\n",
    "!docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have our training and inference container in ECR, we can use it in sagemaker training and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "training_image = '{}.dkr.ecr.{}.amazonaws.com/{}:1.0'.format(os.environ['AWS_ACCOUNT_ID'], region, os.environ['IMAGE_REPO_NAME'])\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    training_image,\n",
    "    role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.m5.large',\n",
    "    output_path='s3://{}/iris/output_byoa'.format(bucket),\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# set the hyperparameters\n",
    "estimator.set_hyperparameters(\n",
    "                        logistic_max_iter=100,\n",
    "                        logistic_solver='lbfgs'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Now run training against the training and test sets created above\n",
    "# Refer to the SageMaker training console\n",
    "estimator.fit({\n",
    "    'training': sagemaker.session.s3_input(s3_data='s3://{}/iris/data/iris_train.csv'.format(bucket), content_type=\"csv\"),\n",
    "    'validation': sagemaker.session.s3_input(s3_data='s3://{}/iris/data/iris_test.csv'.format(bucket), content_type=\"csv\")\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.t2.medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With boto3\n",
    "sm = boto3.client('sagemaker-runtime')\n",
    "from sagemaker.predictor import csv_serializer\n",
    "\n",
    "exampledata = '[[6.7,3.1,5.6,2.4]]'\n",
    "\n",
    "resp = sm.invoke_endpoint(\n",
    "    EndpointName=predictor.endpoint,\n",
    "    ContentType='text/csv',\n",
    "    Body=csv_serializer(exampledata),\n",
    "    CustomAttributes=\"logistic\"\n",
    ")\n",
    "\n",
    "prediction = json.loads(resp['Body'].read().decode('utf-8'))\n",
    "print(\"Classified as {} - Should be: 2\".format(prediction[\"iris_id\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: the running SageMaker container logs can be accessed in Cloudwatch logs for debugging! All print statements errors and stack traces will be visible there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-central-1:936697816551:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
