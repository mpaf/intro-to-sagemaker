{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Built in XGBoost with iris dataset\n",
    "\n",
    "While trying out different features of SageMaker, and as the focus of the workshop will not be in the data science aspects but mostly on SageMaker operations, we will use a simple public dataset called Iris.\n",
    "\n",
    "The dataset contains 50 records of 3 species of Iris each provided in CSV format. In our case we want to predict the species of a flower called Iris by looking at four features:\n",
    "\n",
    "* Sepal length\n",
    "* Sepal width\n",
    "* Petal length\n",
    "* Petal width\n",
    "\n",
    "<table align='left'>\n",
    "    <tr>\n",
    "        <td>Iris setosa<img src=\"images/Iris_setosa.jpg\" width=\"140\"/></td>\n",
    "        <td>Iris versicolor<img src=\"images/Iris_versicolor.jpg\" width=\"200\"/></td>\n",
    "        <td>Iris virginica<img src=\"images/Iris_virginica.jpg\" width=\"200\"/></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explore different ways in which we can use SageMaker to train a model based on this dataset and XGBoost.\n",
    "\n",
    "The first thing to do is to set up a session in order to interact with the SageMaker service. Note that the Studio instance where this notebook is running has an IAM role assigned to it, which we will retrieve in the next code block. This notebook could also be run in your own laptop - in which case you would need to have a AWS profile set-up with the correct credentials to access the Amazon SageMaker service on your AWS account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# this will create a 'default' sagemaker bucket if it doesn't exist (sagemaker-region-accountid)\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "print(bucket)\n",
    "\n",
    "# Get the ARN of the IAM role used by this Studio instance to pass to training jobs and other Amazon SageMaker tasks.\n",
    "role = get_execution_role()\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the role, and created a default sagemaker bucket for storing our training data let's get the Iris data from scikit-learn, use pandas to store it as a Dataframe, visualize the data and upload it to the default SageMaker bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X=iris.data\n",
    "y=iris.target\n",
    "\n",
    "dataset = np.insert(iris.data, 0, iris.target,axis=1)\n",
    "df = pd.DataFrame(data=dataset, columns=['iris_id'] + iris.feature_names)\n",
    "df['species'] = df['iris_id'].map(lambda x: 'setosa' if x == 0 else 'versicolor' if x == 1 else 'virginica')\n",
    "\n",
    "# Let's have a look at the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's describe some statistics about the data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the dataset into Train / Validate / Test\n",
    "\n",
    "We want to split our dataset into a training, validation and test set. The training set is typically bigger, let's use a 70% - 20% - 10% split. We will output the three sets into local CSV files.\n",
    "\n",
    "After that, we will upload the training and validation files to our S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, validation_data, test_data = np.split(df.drop('species', axis=1).sample(frac=1, random_state=1729), [int(0.7 * len(df)), int(0.9 * len(df))])   # Randomly sort the data then split out first 70%, second 20%, and last 10%\n",
    "\n",
    "train_data.to_csv('iris_train.csv', index=False, header=False)\n",
    "validation_data.to_csv('iris_val.csv', index=False, header=False)\n",
    "test_data.to_csv('iris_test.csv', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the dataset to our S3 bucket\n",
    "input_train = sagemaker_session.upload_data(path='iris_train.csv', key_prefix='iris/data')\n",
    "input_val = sagemaker_session.upload_data(path='iris_val.csv', key_prefix='iris/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model on this data with the XGBoost algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker import image_uris\n",
    "\n",
    "# get the URI for the XGBoost container\n",
    "container_image = sagemaker.image_uris.retrieve(region=boto3.Session().region_name, framework='xgboost', version='latest')\n",
    "\n",
    "# build a SageMaker estimator class\n",
    "xgb_estimator = sagemaker.estimator.Estimator(\n",
    "    container_image,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    output_path='s3://{}/iris/output'.format(bucket),\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "# set the hyperparameters\n",
    "xgb_estimator.set_hyperparameters(\n",
    "                        num_class=len(np.unique(y)),\n",
    "                        silent=0,\n",
    "                        objective='multi:softmax',\n",
    "                        num_round=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "s3_input_train = sagemaker.inputs.TrainingInput(s3_data='s3://{}/iris/data/iris_train.csv'.format(bucket), content_type='csv')\n",
    "s3_input_validation = sagemaker.inputs.TrainingInput(s3_data='s3://{}/iris/data/iris_val.csv'.format(bucket), content_type='csv')\n",
    "# Now run training against the training and test sets created above\n",
    "# Refer to the SageMaker training console\n",
    "xgb_estimator.fit({\n",
    "    'train': s3_input_train,\n",
    "    'validation': s3_input_validation\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, from the SageMaker training jobs console, that while it took some time to bootstrap the training instance, you are only billed for the time the actual training took place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus! How to use Spot instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker import image_uris\n",
    "\n",
    "# get the URI for the XGBoost container\n",
    "container_image = sagemaker.image_uris.retrieve(region=boto3.Session().region_name, framework='xgboost', version='latest')\n",
    "\n",
    "# build a SageMaker estimator class\n",
    "xgb_estimator = sagemaker.estimator.Estimator(\n",
    "    container_image,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    use_spot_instances=True,\n",
    "    max_wait=900,\n",
    "    max_run=900,\n",
    "    output_path='s3://{}/iris/output'.format(bucket),\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "# set the hyperparameters\n",
    "xgb_estimator.set_hyperparameters(\n",
    "                        num_class=len(np.unique(y)),\n",
    "                        silent=0,\n",
    "                        objective='multi:softmax',\n",
    "                        num_round=10\n",
    ")\n",
    "\n",
    "xgb_estimator.fit({\n",
    "    'train': s3_input_train,\n",
    "    'validation': s3_input_validation\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an endpoint\n",
    "\n",
    "From the trained model, we will create an endpoint to run inference from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xgb_predictor = xgb_estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.t2.medium'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelname is the actual XGBoost model name\n",
    "model_name = boto3.client('sagemaker').describe_endpoint_config(\n",
    "    EndpointConfigName=xgb_predictor.endpoint_name\n",
    ")['ProductionVariants'][0]['ModelName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Save (Copy & Paste) this modelname for the next session: {}\".format(model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference with example data\n",
    "\n",
    "Now that we have an endpoint up, we can run inference by providing data to it. This is done via a signed HTTP POST request, where the data is in the body. The two simplest way to generate that request and get the inference result are illustrated below:\n",
    "\n",
    "1) With the SageMaker SDK\n",
    "\n",
    "2) With the generic AWS SDK (in this case boto3 as it's python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The inference result should be: 2\n",
    "exampledata = \"6.7,3.1,5.6,2.4\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With sagemaker SDK\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import BytesDeserializer\n",
    "\n",
    "xgb_endpoint = Predictor(model_name)\n",
    "\n",
    "xgb_predictor.serializer = CSVSerializer()\n",
    "xgb_predictor.deserializer = BytesDeserializer()\n",
    "classification = xgb_predictor.predict(exampledata)\n",
    "\n",
    "print(\"Classified as {} - Should be: 2\".format(classification))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With boto3\n",
    "sm = boto3.client('sagemaker-runtime')\n",
    "\n",
    "resp = sm.invoke_endpoint(\n",
    "    EndpointName=xgb_predictor.endpoint_name,\n",
    "    ContentType='text/csv',\n",
    "    Body=exampledata\n",
    ")\n",
    "prediction = float(resp['Body'].read().decode('utf-8'))\n",
    "print(\"Classified as {} - Should be: 2\".format(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the endpoint\n",
    "\n",
    "xgb_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-central-1:936697816551:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
