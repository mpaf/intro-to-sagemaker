{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Built in XGBoost with iris dataset\n",
    "\n",
    "While trying out different features of SageMaker, and as the focus of the workshop will not be in the data science aspects but mostly on SageMaker operations, we will use a simple public dataset called Iris.\n",
    "\n",
    "The dataset contains 50 records of 3 species of Iris each provided in CSV format. In our case we want to predict the species of a flower called Iris by looking at four features:\n",
    "\n",
    "* Sepal length\n",
    "* Sepal width\n",
    "* Petal length\n",
    "* Petal width\n",
    "\n",
    "<table align='left'>\n",
    "    <tr>\n",
    "        <td>Iris setosa<img src=\"images/Iris_setosa.jpg\" width=\"140\"/></td>\n",
    "        <td>Iris versicolor<img src=\"images/Iris_versicolor.jpg\" width=\"200\"/></td>\n",
    "        <td>Iris virginica<img src=\"images/Iris_virginica.jpg\" width=\"200\"/></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explore different ways in which we can use SageMaker to train a model based on this dataset and XGBoost.\n",
    "\n",
    "The first thing to do is to set up a session in order to interact with the SageMaker service. Note that the instance where this notebook is running has an IAM role assigned to it, which we will retrieve in the next code block. If running the notebook in your laptop, you would need to have a AWS profile set-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# this will create a 'default' sagemaker bucket if it doesn't exist (sagemaker-region-accountid)\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "print(bucket)\n",
    "\n",
    "# Get a SageMaker-compatible role used by this Notebook Instance.\n",
    "role = get_execution_role()\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the role, and created a default sagemaker bucket for storing our training data let's get the Iris data from scikit-learn, use pandas to store it as a Dataframe, visualize the data and upload it to the default SageMaker bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X=iris.data\n",
    "y=iris.target\n",
    "\n",
    "dataset = np.insert(iris.data, 0, iris.target,axis=1)\n",
    "df = pd.DataFrame(data=dataset, columns=['iris_id'] + iris.feature_names)\n",
    "df['species'] = df['iris_id'].map(lambda x: 'setosa' if x == 0 else 'versicolor' if x == 1 else 'virginica')\n",
    "\n",
    "# Let's have a look at the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's describe some statistics about the data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the dataset into Train / Validate\n",
    "\n",
    "We want to split our dataset into a training and test set. The training set is typically bigger, let's use a 75% - 25% split. We will output the two sets into local CSV files.\n",
    "\n",
    "After that, we will upload the files to our S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "with open('iris_train.csv', 'w') as csv:\n",
    "    for x_,y_ in zip(X_train, y_train):\n",
    "        line = \"%s,%s\" % (y_, \",\".join( list(map(str, x_)) ) )\n",
    "        csv.write( line + \"\\n\" )\n",
    "    csv.flush()\n",
    "    csv.close()\n",
    "\n",
    "with open('iris_test.csv', 'w') as csv:\n",
    "    for x_,y_ in zip(X_test, y_test):\n",
    "        line = \"%s,%s\" % (y_, \",\".join( list(map(str, x_)) ) )\n",
    "        csv.write( line + \"\\n\" )\n",
    "    csv.flush()\n",
    "    csv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the dataset to our S3 bucket\n",
    "input_train = sagemaker_session.upload_data(path='iris_train.csv', key_prefix='iris/data')\n",
    "input_test = sagemaker_session.upload_data(path='iris_test.csv', key_prefix='iris/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model on this data with the XGBoost algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri \n",
    "\n",
    "# get the URI for the XGBoost container\n",
    "container = get_image_uri(boto3.Session().region_name,\n",
    "                          'xgboost', \n",
    "                          repo_version='latest')\n",
    "\n",
    "#local_session = sagemaker.LocalSession()\n",
    "# build a SageMaker estimator class\n",
    "xgb_estimator = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.m5.large',\n",
    "    output_path='s3://{}/iris/output'.format(bucket),\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "# set the hyperparameters\n",
    "xgb_estimator.set_hyperparameters(\n",
    "                        num_class=len(np.unique(y)),\n",
    "                        silent=0,\n",
    "                        objective='multi:softmax',\n",
    "                        num_round=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Now run training against the training and test sets created above\n",
    "# Refer to the SageMaker training console\n",
    "xgb_estimator.fit({\n",
    "    'train': sagemaker.session.s3_input(s3_data='s3://{}/iris/data/iris_train.csv'.format(bucket), content_type=\"csv\"),\n",
    "    'validation': sagemaker.session.s3_input(s3_data='s3://{}/iris/data/iris_test.csv'.format(bucket), content_type=\"csv\")\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, from the SageMaker training jobs console, that while it took some time to bootstrap the training instance, you are only billed for the time the actual training took place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an endpoint\n",
    "\n",
    "From the trained model, we will create an endpoint to run inference from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xgb_predictor = xgb_estimator.deploy(initial_instance_count=1, instance_type='ml.t2.medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Endpoint name is the online inference endpoint\n",
    "endpoint_name = xgb_predictor.endpoint\n",
    "\n",
    "#Modelname is the actual XGBoost model file\n",
    "model_name = boto3.client('sagemaker').describe_endpoint_config(\n",
    "    EndpointConfigName=endpoint_name\n",
    ")['ProductionVariants'][0]['ModelName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Save (Copy & Paste) this modelname for the next session: {}\".format(model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference with example data\n",
    "\n",
    "Now that we have an endpoint up, we can run inference by providing data to it. This is done via a signed HTTP POST request, where the data is in the body. The two simplest way to generate that request and get the inference result are illustrated below:\n",
    "\n",
    "1) With the SageMaker SDK\n",
    "\n",
    "2) With the generic AWS SDK (in this case boto3 as it's python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The inference result should be: 2\n",
    "exampledata = \"6.7,3.1,5.6,2.4\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With sagemaker SDK\n",
    "from sagemaker.predictor import csv_serializer,RealTimePredictor\n",
    "\n",
    "xgb_endpoint = RealTimePredictor(endpoint_name)\n",
    "\n",
    "xgb_endpoint.content_type = 'text/csv'\n",
    "xgb_endpoint.serializer = csv_serializer\n",
    "classification = xgb_endpoint.predict(exampledata)\n",
    "\n",
    "print(\"Classified as {} - Should be: 2\".format(classification))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With boto3\n",
    "sm = boto3.client('sagemaker-runtime')\n",
    "\n",
    "resp = sm.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='text/csv',\n",
    "    Body=csv_serializer(exampledata)\n",
    ")\n",
    "prediction = float(resp['Body'].read().decode('utf-8'))\n",
    "print(\"Classified as {} - Should be: 2\".format(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the endpoint\n",
    "\n",
    "xgb_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
